And let's go and run this so we can see the print from the beginning. And we can see also the print inside the search function, the search tool. And we can see that the query that was actually sent is called weather in Tokyo. And now we get the response. We have tons of things in here in the response. So by the end of this course we'll know most of them. What do they mean. And let me go and let me run this in debug mode so we can see this more nicely. And we can figure out what's the actual output of the agent here. So all right so let me put a breakpoint here and if I'll check out the results variable. So it's a dictionary containing the key of messages. Here the first message is going to be the input. So you can see it's a human message with the content of what is the weather in Tokyo. And the answer is in the last message, and we can see it's labeled as an AI message, where it says that the weather in Tokyo is currently sunny. Now, what's in between? We have here an AI message and a message. And those are actually what is the agent is doing in order to get this answer. So right now it is sort of like magic. But in this video you'll have a better understanding how this is working. And by the end of this section and the next section, you'll know exactly how this is working and there will be no magic. Okay, so trust me on this. Cool. So let me now open links and let's see the trace of this agent execution. And let's dive a bit deeper of what's happening here. Now you can see now that the title here of this execution is called Land Graph. And this is because underneath the hood in the internals, it's going to be executing via the graph framework. Do not worry about this I promise we will be covering this in the course we open now the first call. So it's called here chat open AI. And by the way something interesting that the default model for link chain currently and this is was filmed on November 7th, 2025. So the current A model is GPT 3.5 and not GPT five. So this is just something interesting right. So let's analyze this LLM code. So here we can see the input to the LLM call. And we sent a message of what is the weather in Tokyo. Now when long chain made the LLM call. It also sent to the LLM information about which tools does it has access to. And it can call. And if you open this model you'll see all the information about the tool that we defined earlier. Now if you've noticed my terminology, I said that we send to the LLM the list of tools and I said LLM. And I did not say agent because right now we are in the granularity of the LM with the LM call here. So we equip it now our LM with a tool that it can choose. And with the query of what is the weather in Tokyo, it decided that it needs to invoke the search tool with the query weather in Tokyo. And we have here some kind of tool calling ID here. So notice here the LM response is not. The invocation of this tool is simply which tool to call and with which arguments here. And it's going to do so with function calling. And how does this work under the hood? We are soon in this section. And in the section later we're going to figure out. Alright, so after the LM decided that it needs to call the search tool link chain then went and invoked the search tool with the query Weather in Tokyo. And this we can see right over here. And we can see that the response of these tools execution is Tokyo, weather is sunny and this is the static string that we defined in our Python function earlier. So we went and it ran this function with the argument of weather in Tokyo. So let's recap what happened up until now. So the LM decided which tool to call with which arguments. Then link chain went in ran this function with those arguments. So something very interesting is happening right now. And let me give you a quick preview of what's to come. So we have here a reasoning engine that decides which tool to call and with which arguments. And we have here now an agent execution runtime which actually goes in execute and run those tools and gets the output. Now link chain is going to be making another LM call, but this time it's going to contain the input. It's going to contain the LM decision to call the search tool, and it's going to contain the answer after executing the search tool. So the LLM should have a very easy time now generating the answer, because it has all the information it needs. And we can see here that the answer is that the weather in Tokyo is currently sunny right now. Remember that I told you that now all of those messages are going to be making more sense. The first message was the input. The second message was the AI's decision to call the search tool, and with which arguments link chain went and executed that tool. And then the result of this tool is structured here in this tool message object. And the tool message is simply a data structure to represent a result of a tool execution. And in this case the result of executing the search tool. And finally link chain went and made another LLM call with the input with the decision of the LLM to execute the search tool with the search tool a result. So the tool execution results. And then the final answer is that the weather in Tokyo is sunny. And also notice that the final LM call did not choose to execute a tool, but it chose to return an answer because it had all the information it needed. All right, let me go back to the code now and let me change the model to be GPT five and let me run it again. Now notice this time it's going to run a bit slower, because GPT five is a slower model than GPT 3.5 turbo. And we got a result. Let me go now and show you the trace now of the latest run. And it's very similar. Just notice here there is a difference because the model name now is going to be GPT five. And let's go now back to the code. And let's go and add more meat here. Now to this search agent.