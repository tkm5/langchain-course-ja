Let's start now by defining the list of tools we're going to give our AI agent. So it's going to be a search agent. And it's going to be a list containing only one tool, which is going to be the search tool. Now I want to discuss what is this search tool object. Now in the previous video I showed you the search API. So how to use it in the AI playground. Now they have an SDK. We can programmatically use their API. All we need to do is to generate an API key and to use their SDK. Now if we want to use an LLM as a reasoning engine and to kind of get this behavior where the LLM has access to this API, to this SDK, then we need somehow to communicate to the LLM that it has in its disposal this kind of capability. So the ability to use a API right. So we need to tell the LLM maybe what is this tool name to use it. What arguments does it received? How does the output of this tool look like. So we need to give all this information to the LM. And this is exactly why LinkedIn created the tool class. Now the LinkedIn tool is a component that will allow the LMS to interact with external utilities or APIs by generating structured calls to those tools. And they're defined with the schema that is going to specify the name, the expected arguments, and it will allow the LM to generate calls with the structured inputs. Now the model does not execute the tool itself. Instead it only produces the arguments for the tool call. And the system running length chain will execute the tool and propagate back the output to the model. And this is very important. And this is what's going to happen actually in the agent executor. This is this is a quick spoiler. Now we can convert any Python function to a tool using the tool decorator which then the model can call by generating the appropriate arguments for the tool call, and it's going to include the tool name, the dictionary of arguments. And it's going to enable structured and reliable interaction between the LM and the external API. Now how will the LM know how to use this tool and when to call it. And usually the tool description will tell the LM what this tool is doing. So it will know when to use it. And in that LM call will also provide what arguments that this tool receive. So what are the names. What are the types. So eventually the LM has all the information of what it has in its disposal. What are the tools when to invoke them and what are they expecting and what are they outputting. All right. So back now to the code. The search object is from the link chain to build integration. And this is where the team, they took their SDK and they wrapped it as a link chain tool and basically created an object that is going to hold the description of this function. It's going to hold the arguments that it needs and the return value of this function. And that's what's going to be communicated to the LM. All right. So let me now run this in debug mode. And I want to examine the tools variable here. And I want to show you this tool object. So let us wait until we hit that breakpoint. And when we hit it let's open now the local variables here sorry the global variables. And we can see now we have the tools list here. It has only one element and it's called search here. Now this object here if we'll check out its attributes we can see we have an attribute called description. And here we can see description is a search engine optimized for comprehensive, accurate and trusted. And this text goes on and on. And this is the description of the search tool. So this is one thing that's going to be propagated to the LM column. So the LM would have this description of this video. Now we need to tell LM what kind of arguments these two receives so it can create the tool call. So here we can see we have here a dictionary where the keys are going to be the parameters that the search is going to receive. So we can see we have here query include domains exclude domains and other business logic that really may require. Now it's super important to note that the description and the arguments description, it's super important for us to be explicit. So if this is going to be ambiguous, the LM is going to have a hard time to crafting with the tool. And this is, by the way, one of the reasons why I used the link chain integration instead of simply importing the search client with the SDK and wrapping it as a selection tool, simply because the team knows their product the best and they can write the description for their arguments and for their function. A much better than I can write. So here you can see a bunch of other fields and that are the arguments. And, uh, yeah, this is pretty much it. Let me go show you. Now, um, this is the rest of the tool description here. Let me go show you the tool name. The tool name is going to be right over here. So we can see tool name is underscore search. So just to conclude we can think about a tool as a wrapper of a Python function that's going to have the fields of name description arguments and maybe some extra metadata. But that's pretty much it right. So and this is what's going to be propagated eventually to the LLM. And we're going to see this in action very soon. All right. So let me now go back to the code and let's go and examine the actual implementation of the search class here. And we can see that it inherits from base tool. And this is a link chain object. And this is going to have the interface of a having the attributes of name description arguments that we talked about earlier. And here in this description we can see some examples of how we can instantiate this object here, which is going to be a link chain tool. So we can also fixate some of the arguments. So there will be fixed when calling a search API. And here we can see for example the name. We can see the description and we can see the schema. So it's going to describe what kind of arguments it's going to receive. And some other descriptions of the other arguments that it's going to receive. Now when it's going to be executed then it's going to execute this underscore run function. And it's going to use the SDK to make the API call to the service. I remind you, the LM is not going to make this call to the API. It's going to only reason and decide that we need to call it. So the actual call and execution of this method is going to happen in our link chain application. So that's going to be in the agent executor which we're going to check out later.