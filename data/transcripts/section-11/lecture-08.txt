So in this video we're going to implement the react py file. And this is going to hold all the reasoning logic which we're going to use in our graph. So the TLDR for this is that we're going to use an average function calling that is going to be the reasoning engine which decides which tool to call. Let's start with the imports and lets me import from env the load function so I can load the environment variables and all the API keys I want to import from link chain core. I want to import the tool decorator. So this will help us to create a tool from our functions that we define. I want to also import from link chain OpenAI the chat OpenAI. So we can make LLM calls to GPT. We want to import from link chain Tavileh the search object which is a pre-made search tool. So we can use and plug it in to our llms to our agent. And now let's go and load our environment variables. And this is time to begin the coding. And we want now to implement the tools. So for this example we'll be implementing the triple tool which is a function which takes in as an input an integer a float. And it's going to return us this float triple. Now notice here all that I'm doing is writing the prefix here of the function. And I'm going to be auto completed by cursor. This is simply, by the way, mind blowing how coding has changed over the years. And with LMS how easy it is, um, and how convenient it is to to implement those kind of things. So we can see here an example implementation. Let me remove this unnecessary code here. And we have here the triple function we have here the description of this function which is going to be propagated eventually to the LM. And the LM is going to decide whether to use this function or not. Anyways so this is the triple function. Now if we want to turn it into a link chain tool, All we need to do is to use the decorator of tool here that we imported, and we get a link chain tool that we can plug in. Alrighty. So now we want to equip our agent with not just the triple tool, but we also want to give it the search tool as a tool that it can use. So in order to do that let's define a new variable. It's called tools. And tools is going to be a list which is going to be an object of the search prebuilt tool and the triple tool that we just implemented. So the first element of this list is this object of this class that we search initialized with the max results equals to one. So in this example we're simply limiting the search to fetches only one search result. And by the way this search tool has already the description prebuilt in case you were wondering. So the package maintainers implemented the search description. And this is going to be propagated eventually to the LM. And the second element of that list is the triple tool. All right. So now we want to tell the LM somehow that it has access to this kind of tools. All right. So now I want to discuss the reasoning capabilities of Dlrm. So how does the LM decide which tool to call. So by now it should be familiar with the algorithm and the react prompt. And this is a very fancy prompt that is a derivative from the react paper which is helping to leverage the reasoning capabilities of the LM. So this is how everything started. And in the early days of agents this is how it was used. People used this special prompt in order to leverage Llms to decide which tool to call. And by now things have evolved and we have better ways to do things which, by the way, all derive from this very special prompt. What I am referring to right now is function calling, and function calling is a feature in most modern llms today that allow us to give the LLM when we initialize it, the definition and the instructions and the details of the tools that we want to give. The LLM and the LLM is going to return us in the response, whether we need to call those functions with which arguments if it needs to. And I elaborate on function calling in this course, in this video and in this video, and we are actually not exposed to the implementation of function calling in LLM vendors. So each LLM vendor has its own way of doing things. The implementation is probably a special system prompt, which is similar to the react prompt and helps the LLM to decide whether it needs to choose a tool or not, and returns the answer in the correct format. However, now the LLM vendor is responsible to parse the the LLM response and to arrange everything nicely in the response we're getting and putting the function call in the function call key in the response from the LLM. And again, don't worry if you don't understand this in full. All you should know and all you should take right over here that this is the modern way to choose tools. And if we're going to use function calling, we're basically offloading the task of choosing the correct tool to the LLM vendor. And we don't need to handle this parsing like we did from before, because the LLM does it for us. So for us, it's a very big win because it's less code that we need to write. And actually the results and the performance of this is only improving over time because all the LLM vendors, they have dedicated engineers that are defining this and are making sure that we get quality results and the correct tools to be called and the correct arguments. All right. So right now we want to implement the reasoning capabilities of our agent. And unlike we did before, we're not going to use the react prompt. We have something which is better and this is function calling. So we're going to initialize an LM that supports function calling. And we're going to give that LM the tools that we defined. Now the search tool and the triple tool both are link chain tools which have the descriptions of which arguments do they receive. So I'm going to initialize a chat open AI. And I'm going to use the bind tools method and supply the tools that we already wrote. And link chain is going to take the tool descriptions and it's going to send that to the LM. Every request that we're going to make. So the LM can now return us this function call or tool calling field with the correct function to be called. And we don't need to handle any parsing because the LM vendor handles the parsing for us. And if there is a function call we get it in a special key in the response. And for more information about the implementations of function calling, please check out those videos over here. Cool. So this is it for this video. And I want to run this, um, script right over here. Let's see that everything runs even though we're not running anything right now, just to see we don't get an error and we can see it's working. And let me go and commit the code where you can find it. So let me write git add. And we have here this file that was added right now. So let's go and commit it. And I'll call the commit function calling reasoning. And let me push it. And let me show you the repo. And if you'll go to the commits you can see you have this commit right over here where you can see the code. And in the next video we're going to implement our lane graph nodes. And those are the executables which are going to run during our execution.