So now it's time to cast this function into a long chain tool. And we're going to later equipped our agent to have the ability of using this tool. So in the previous video, we defined our tool manually by using the tool class of chain. But we can also use a tool decorator that will take a function and convert it into a long chain tool. So for that I'm going to import the tool function. And I'm simply going to use this decorator above this function. The tool decorator is a long chain utility function that will take this function and create a chain tool from it. It's going to plug in the name of the function, what it receives as arguments, what it returns, its description, and populate it in the chain tool class. So if I'm going to debug this code and put a breakpoint right over here and let's now inspect this function, get text length. We can see now it's a structured tool and we have the name and we can see and have its description which includes the function name, its arguments, what it returns and the description that we wrote. This will be used by the LM reasoning engine to decide whether to use this tool, and we'll soon see how it works. Just one minor change we need to make to our getText function. So sometimes the LM may pass as our text some unnecessary characters which are not alphabetical. So for example backslash n or some single quote. So here I'm simply removing those and cleaning it up. And all the code for each video in this section is available in the videos resources. Now, I highly suggest that you use this implementation of the function in the video, even though I forgot to add it. Cool. So now I want now to stop it and let's create a new variable and let's go and call it tools. It's going to be a list of long chain tools, which now is going to be populated only with get text length. Now this list of tools we're going to supply to our react agent. So remember when previously the agent selected the correct tool to use from our prompt. And it was sort of like magic. So now we're going to dive deep and to see how it's actually implemented and what's happening under the hood. Okay. Now it's time to go and see this in action. And all of this tool selection basically is going to happen from a very special prompt. We're going to be writing. So I'm going to write here the template variable. And now I'm going to attach here a very smart prompt that will make our tool selections. So you can find this prompt in the source code. And we'll do it later in this video. However right now I want to go to the Lang Chain Hub, which is a part of Lang Smith, and the Lang Chain Hub is a marketplace for prompts. It's all free, and people share their very well crafted prompts, including the prompts for this react agent. Now, this prompt was created by Harrison Chase, the creator of Link Chain, but everyone can submit prompts and share them on the chain hub. Anyways, I'm going to open my browser and I'm going to search for Lang Chain Hub. I'm going to go to the second link, which is a part of Lang Smith. And right over here we have the list of prompts. And I'm going to filter by agents and I'm going to write here react. So right here we have a prompt that was crafted by Harrison Chase. You can see it right here. And we can see the prompt right now. So on the right side you can see when it was last committed. When was the last change. You can see what kind of type this prompt is and the number of downloads it has, how many followers it has. Because prompts is something that is dynamic and people always refine and refactor their prompts. And right now we're simply going to copy and paste it into our IDE. So I'm going to click the copy button and going to paste it. Let's just format it a bit more nicely. I'm going to take all of those lines over here and going to indent it in the react algorithm. This prompt is going to be sent to the LM. And this is going to generate the thought of the LM which is going to help us select the correct tool. So let's analyze this prompt. This prompt is in my opinion, the most important prompt in LM application development. And it's an implementation of the react paper. And if you want to find more about the theory behind this prompt, I highly suggest you going over the theory section where I analyze prompt engineering techniques from the theoretical point of view and analyze their research papers. And you can find a video about react. Anyways, let's analyze this prompt. Answer the following questions as best as you can, and you have access to the following tools where we have a placeholder for tools. And here we're going to plug in our list of tools and the tool descriptions. And it continues. Use the following format. Question the input question. You must answer thought. You should always think about what to do. And then we have action and action input. And the action is going to be the action to take and should be one of the tool names. And here we're going to plug all of our tool names. And the action input is going to be the input for that action. So in our case our action is going to be get text length. And our input for it is going to be the text that we want to count the length of. And after that we have an observation which is going to be the result of the action. So this is going to be the result of the tool we ran with the tools input. Now we can see here that this thought action action input observation can repeat n times. And this is referencing the iterations of the agent when it's deciding which tool to use. Uses that tool then keeps going and doing this over and over again. So the final answer would be derived from the following format. Thought I know the final answer and final answer column. The final answer to the original input question. So here we have the output indicator begin. And our question which is an input placeholder. And then thought agent scratchpad. So in the thought agent scratchpad we're going to discuss later in this video. And from a quick analysis of this prompt we can classify this prompt as a chain of thought prompt. Because we are asking the LM to tell us how it's thinking and how it's coming up with its answer. It's also a few shot prompt because we're supplying examples and telling them, how do we want our format output to be with. And this is an implementation of the react paper Reasoning in Acting. And again, if you were interested in the theory of all of this, you can check out the theory section where I dive deep into this. Cool. So since we'll be dealing with the Agent Scratchpad only later in this video. I'm going to remove it. And this is our prompt now that we're going to use. And this is how our agent is going to select the correct tool to use. So from our template we want to create a blank chain prompt template. So I'm going to write a prompt variable. And it's going to be an instance of prompt template. And I'm going to import this link chain class. And this prompt template is going to be initialized from template function. And here I'm going to supply it and plug in the template. Now I'm going to use the partial method. And this method will populate and plug in the values of the placeholders that we already have, because we know we have the tools and the tools name. So we can plug it in into our prompt right now. And the input variable placeholder will come dynamically when we run this chain. So it's going to be from the user. So because we already know what we want to plug in in the tool names and list of tools. Then let's do it and initialize it with this partial method. So we want to initialize the tools placeholder. And here we're going to put the list of tools that we had already created. And we want also to plug in the value of tool names and tool names is going to be a comma separated tool names list. So I'm going to use the join method with the comma separator. And I'm simply going to iterate over all the tools. So for t in tools. And I'm going to access the tool name. So this eventually will print a nice string of the tool names concatenated together separated by commas. Now we're almost done with the prompt template. But in the tools keyword I can just plug in the list of tools that we gave, because this is a list of Lang tool objects, and we want here to supply a string which will represent the tools description because Lmns receive only text as input. So luckily for us, Lang Chain already implemented this logic that converts a tool list of tool objects into a string that's representing their description. So instead of passing tools, we'll call the function render text description. And let's now import it from Lang chain. And we're going to plug here the list of tools. So if we'll examine this Lang chain utility function we can see all it's doing is iterating through the tools we gave it. It's formatting nicely into a string the tool names and tool descriptions. That's it. So we're going to eventually supply a string into our LLM. So let's run it for a sanity check to see that it also runs smoothly. And we can see it worked with no errors. And now we want to define our LLM which is going to be our reasoning engine. So I'm going to use chat OpenAI the chat version of GPT 3.5. We're going to be using temperature equals to zero because we don't want any creative answers. And now we're going to plug in this top argument. And here the values are going to be backslash n observation. And so this will tell the LM to stop generating words and to finish working once it's outputted the backslash n observation token. And why do we need it. Because if we want put this top token then the LM would continue to generate text. And it's going to guess one word after another observation. And the observation is the result of the tool. And this is something that will come from running our tool. And if it comes from the LM then this is simply an hallucination. And it's not really the real time result from running the tool. Now, I know that for some of you it may sound kind of abstract, but I promise you, in the series of videos we'll be seeing this with our own eyes and you'll understand the importance of using this stop token when we use the LM.