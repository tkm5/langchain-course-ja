Instructor: By the end of those couple of videos, we'll be implementing ourself the create react agent function, and specifically the agent executor. So what is the agent executor? It has a very fancy word and it sounds very, very cool and very complex, but essentially it's a while. So this is a fake implementation of the agent executor. Now this is just the pseudo code and it's essentially a while loop that keeps running, takes the user input, it takes this prompt and augments it with all the information we have about the tool for the LLMs disposal, and then sends everything to the LLM. Now the LLM returns as an answer, and if the answer contains that we need to run a tool. Then we figure out, and we'll talk about how this is done, which tool to run, we then run this tool, and then we go over and over again. So when do we stop? When the LLM decides that it has defined an answer or by default if we limit the number of iterations or number of messages we want to send. This is pretty much it, a fancy while loop where we are keep making LLM calls, then interpreting what the LLM has responded and then choosing the correct tool to use, running it, then running it over and over again. Now if we think about it a little bit deeper, we'll figure out there is a little more stuff that we need to perform during this entire process. So we first take the user query to the agent, and we then make this special LLM call. Now let's say the LLM is able to determine that we want to use the Get Text Length tool, so it gives us an answer back, but the LLM gives us text, so we need to parse this text in order to work with it. After we do that, we choose the correct tool the LLM chose for us, and we execute this tool and we get a response. Then we take this response alongside with our query and we run another iteration. So now the agent can decide whether to use the tool or another tool, or whether it has enough information to return the answer. So that's pretty much it, what's happening over and over again. And in the next couple of videos we'll be implementing all of this ourselves. I'm going to head up to desktop and I'm going to create a new directory called React Langchain. I'm going to cd into this directory and I'm going to create a new virtual environment by writing pipenv shell. The environment is created, and I'm going to install with pipenv langchain, OpenAI since we'll be using GPT-3.5, I'm going to install black formatting tool and Python.n package to help us load environment variables. Amazing. So now that we have all the packages installed, let's go and head up to PyCharm and you can use VS Code if you want. And I'm simply going to open the directory that we just created. So notice that PyCharm automatically detected our pipenv environment. So we're all good. And now let's create a new file and we'll call it .env and it's going to hold our environment variables, which we're going to store it with the OpenAI API key. Cool. So now that we've done that, let's go and create a new file and we'll call it main.py and here we'll be writing all of our code. And we want to start by loading our environment variables, which right now is the open AI API key that are stored in our .env file into our Python environment. So for that we're going to be used the Python dotenv package that we installed before. And we're going to do it with two simple commands. We're going to first import from dotenv the function load dotenv and we're going to call it and it's going to take all the values from the dotenv and load it into our environment variables. So now let's run some boilerplate code. I'm going to write if name equals main. And for our beginning print I'm going to write, print("Hello ReAct LangChain!") I'll go and click the green play button, which is currently configured on the current file, which is our main.py file. And this will run our script. So let's run it for a sanity check to see if that everything is working, and we can see the print, and we can now start coding. We'll first begin by writing our first tool, and for that we're going to be defining a function, which is called Get Text Length. And this function is going to receive text, which is a string, and it's going to return an integer. Now we'll start by defining what this function is going to do. So the description here is very important because this is going to help the LLM decide if it's going to use this tool or not, in its reasoning engine. And we'll soon see how this actually works. So I'm going to write the description of this function. I'm going to write that it returns the length of a text by characters, and the implementation is very simple, it's simply returning the len function when we inputed the text. So it will count the number of characters. For sanity checks, let's simply run it, and let's put it on a simple input of text equals to dog. And we can see that dog indeed has three characters in it. So we can see the function is working.