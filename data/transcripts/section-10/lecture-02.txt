Instructor: So let's have an overview of what's my take in the LLM applications landscape. I like to classify it into four main categories. The first one is company slash people that are writing LLM applications, which all boils down to a simple LLM call. Those applications slash features simply send inputs to the LLM, get a response back, maybe manipulate it a bit, but eventually display to the user. And that's it. Very simple, not sophisticated, but a lot of time can bring a lot of value to customers and to users. For example, an application I liked is this application that creates children's stories. When we give it the subjects and the topics. It will send those to the LLM and create a children's story with cartoons and with pictures, and it's very cool in my opinion. But the overall implementation is pretty simple. More advanced LLM applications will incorporate some kind of vector store and use the retrieval augmentation generation pattern with semantic search in those vector stores in order to get the relevant chunks, the relevant data to answer super specific domain related questions. For example, an application that I like is called Quiver, and it's also called the second brain, where you simply dump all of your information, whether it's PDFs, whether it's databases, whether it's videos or chat history, and it simply index them the information in a vector store and uses the RAG pattern, the RAG pattern with semantic search in order to QA over our data. So the idea behind this application is that you simply dump it all and then you can chat with it. Now, if we want to take our application up a notch as far as LLM complexity, we can incorporate agents and leverage the LLM reasoning engine in order to run non-deterministic code and basically have an agent that will decide which tools to use when it's the most appropriate. An interesting use case I saw is of a cybersecurity company called Torq, where they created an agent called Socrates which resolves and remediates alerts with non-deterministic steps. So it reads the alert information and then decide how to remediate it by utilizing the security tooling already connected to the Torq hyper automation platform. Excellent example of how to utilize agents for real world issues like cybersecurity. Now, the last pattern I wanna talk about is combining agents and vector stores with semantic search. So projects like AutoGPT, like GPT Engineer, they incorporate vector stores to implement something which is called a long-term memory and are using semantic search in order to achieve very advanced capabilities. Those capabilities can be mimicking human behavior agents talking and interacting with each other and solving complex tasks. Now, all those projects, AutoGPT, GPT Engineer, Baby AGI, they're in the very, very beginning and they're pioneering what's called autonomous agents. So to summarize this session, my main point was that we can classify every LLM application today to one of those categories. My goal in this course was to teach you how to implement those patterns yourself. So we learned about agents and we learned about vector stores and how to interact with the LLMs and the theory behind it so you can go and build your own applications.