So in this video, we're going to dive deep into the context flow when using subagents and we're going to understand why subagent are so powerful and so useful. We have here the main agent thread, and this is our main conversation. And every message that we're going to be sending here is going to increase our token size. Now, when the main agent decides that it's going to use a subagent, it's going to create a brand new prompt, and this prompt is going to be passed into the subagent. And this is the only context that the subagent sees when it start to execute. So it's not aware of the entire conversation of what happened so far, it's just aware of the prompt that the main agent has generated to us. And I'm going to show you a way where we can manipulate this prompt where the main agent is going to prompt the subagent such that the subagent is going to have much easier time working and we're going to get better results. So we can totally influence this prompt over here. So when we start to execute the subagent, it's only as good as this prompt which is going to be passed into it because that's the only context that is going to receive. And it's going to work independently. It's going to invoke the tools that it needs. It's going to maybe do some integrations and at the end, it's going to return one condensed response to that main agent. And every time we're going to spawn a new subagent, then we're going to start with a fresh context with only this prompt being sent. Now you can see the advantage here is that we can run the main thread and we can keep it lean. And if we're going to use subagents, we're going to delegate a lot of context to those subagents, which are going to work in isolation, and we're only going to get that response, the artifact, which is going to be prompt back into this main agent. So in this way, the main conversation is going to have much easier time maintaining the lean context and we won't be needing to use the slash compact or slash clear command because we know that the more context we have in our main agent then performance degrades. So this is a very useful tool to see. All right, so just to summarize, we have our main agent, which is going to give one input to the subagent. Then the sub agent is going to do the work and is going to output one output to the main agent back again. And this is a very smart way to compress our context. All righty, now let me show you this in action and let me show you why this is so important. And here, I'm going to illustrate the context window as our conversation with cloud code progresses. Now, as you know, LLMs has token limits. So we are kept with the number of tokens we can send them. Now we can use a model with 200K or with 1 million tokens or in the future maybe it would be 2 million or 10 million, but this number is finite. Now obviously, we do not want to reach the token limit because one, if we surpasses then our request is going to fail to the LLM, and even if the request didn't fail, then the answer we going to get back, it's going to cost us more because every token is going to cost more, and it's going to be slower because we have more tokens. So the latency is going to increase. And most importantly, if we're going to get near this context limit, it's pretty safe to say that we're going to encounter some context pollution and we're not going to get the results that we want because we have tons of contexts, which is maybe not relevant for the task, and really this is not optimal. So we want to keep our context lean as much as we can here. And in our interaction with cloud code, every turn that we make, every message that we send, it's going to consume tokens and it's going to add tokens into our context window. So maybe in the first turn we added 10k tokens then in the second it increased to 30k. And by getting to the fifth turn, we reached 100K tokens. And if we're going to be using one instance of cloud code, then in some point or another, we will be needing either our compact window with the slash compact command or we would be needing to maybe clear everything or even open up a new instance of cloud code and start fresh. And that way we're going to use our history. And the only thing I want to demonstrate here is that we are limited in the context and we can't simply use one cloud code instance to do everything that we need. The context limit is limiting our interaction with cloud code, and this is my main point here, but with subagents, it's a very elegant solution to maybe increase this limit. And every time we're going to be using a subagent, it's going to be running with its own context window and every token is going to consume every token that it's going to be using, it's not going to be calculated in our main cloud agent. So by the end of this sub-agent execution, it's only going to return one condensed response. So it may be 15K or 20K tokens with the summary and the code that they changed. But the point here is that we won't be accumulating these context here, and you can see how elegant this is for our context engineering efforts. And each side chain, each sub agent, which is going to run, it's going to be running with its own system prompt, which we tailored made according to that need, so it would be able to solve a task that it should do in a better way than the main agent. That's the entire point of subagents here. So this is super, super elegant, this idea here, and it's very, very powerful.