Presenter: I want to introduce you to a super useful place called The LangChainHub. It's supposed to be a single stop shop for sharing prompts, chains, agents, and more. So it's a repository for those prompts. Often the secret sauce for getting good results from the LLM is a high quality prompt without prompt engineering, and a lot of techniques that we discussed in this course, some of them in order to get more from the LLM. Now, the LangChainHub is a place where it offers a collection for commonly used prompts that people can share together and download. So I'm going to search for LangChainHub, and I'm going to click this URL and I'm here, and it's a part of LangSmith Notice that LangSmith is currently in beta version, so you can sign up for the beta. I'm not sure how fast the approvals are, but it's planned to be released soon. So we can see that here we have a repository of prompts that we're going to explore. So we can see that they're sorted by use case, whether we want prompts for agents, for autonomous agents, for classification tasks, for code writing, for anti extraction, for self-checking, SQL, you name it. All the common use cases are here, so we can filter them up. Now whether it's a model from Google, from Meta, from OpenAI, when working with models, we need to optimize for that specific model. And the same prompt that we wrote for model A may not be optimized for model B. So some model may accept words that are not accepted by other models, for example. So we need to optimize the prompts according to the vendor. Let's now explore some prompts. Let's focus on QA over documents. And let's take a look at the retrieval augmentation generation prompt. Now this piece of code shows how to use it and how to use this prompt. So we can see at the beginning we're loading the documents, splitting them, putting them in the vector store, and then we can see how we actually download this prompt. We use the hub.pull method, and we simply put this URL. Now you can see that the prompt variable is being passed to the retrieval QA chain in the chain type query arguments. So notice here you can change the actual prompt that is being called to the LLM augmenting the original prompt with any custom prompt. So this is very important. It can be customized. Right over here you can see the actual prompt and what parameters does it accept. And at the bottom you can see another example of how to use it. Something interesting is that in the right side you can see all the commits of how this prompt changed over time. And if you even want, and this is another view of the commits, and if we go back over here and let's take a look for other prompts, we can also sort by popularity, by top download. And you can see the number of downloads and likes and comments and how many people are watching whether this prompt is changing. So this is the React Chat example. So this is an agent based prompt. So it's for selecting tools. Now you can open it in the playground and you can plug in the parameters and see how this prompt behaves and what results do you get from it. So it's a easy tool, and very intuitive tool to use when experimenting with new prompts. So as you know, prompt engineering is a very big part of a writing an LLM application. So you can see how this prompt behaves with different vendors with different parameters of temperatures, lengths, and penalties, for example.