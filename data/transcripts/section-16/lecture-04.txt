Instructor: All right, so I'm in the MCP doc official GitHub repository, and here we have an explanation and diagram of how this MCP server works and what does it actually do. It's going to leverage llms.txt of documentation of public documentation of packages, for example, LangGraph has one and it's going to help our agent, whether it's Cursor, Windsurf or Claude desktop, going to give them the ability to fetch the most up-to-date documentation. So this is very useful because you know, documentation of open source project, especially in the generative AI field, they change constantly. And if we'll index them manually, then what will happen, they will get out-of-date very, very quickly. So this actually gives us the ability to fetch them dynamically because we're going to scrape them from their official website, which is supposed to be up to date. So how does it really work? So first it has access to this llm.txt tool, which has a bunch of URLs and explanation of what do those URL represent, which topic do they cover. And if we'll take an analogy to a book, it's going to be the first page of the book with the index of every chapter and a quick explanation of what does this chapter cover. So this is the first step of the MCP server. The second step is to figure out which URL do we need according to the user's question and then to fetch that relevant information by simply making a curl request. So if we'll use this MCP server, no more stale documentation. So to run this MCP server, it's implemented in Python. So we'll download UV if you don't have it installed. And now we want to go and open the terminal, clone this code here and install the dependencies. So let's go and do that. So let me go here and let me open the terminal. I'm going to go to my directory of my MCP servers and let me just go and clone that. So we're simply cloning the code of the repository we just saw. All right, let's go and cd into it. And now let's go. And according to documentation, let's go and install the dependencies because it's using UV. So first I'll create a virtual environment and I'll activate it so you can see now on the left side here we have mcpdoc. So this is my virtual environment. Now we need to install the dependencies in all the relevant packages. And by the way, all the dependencies are going to be described in the UV.log file over here. Alright, so all of them installed and let's go and take the UV path. And I wrote which UV in order to give the full path of the UV execution file. So let's go back to the docs and let's first test and have a sanity check of this web server. And here we have the command to run it locally with an llm.txt of our choice. And here it's using the LangGraph llm.txt that we showed earlier. So let me just copy that and paste this command over here and it should run locally the MCP server in port 8082. And we can see it's running. All right, let's create another instance of terminal. And here we want to run MCP inspector to debug a bit this MCP server. And let's go down in the formal GitHub a repository here and we're going to have here the command to run inspector. So let's go simply paste it NPX and to run the inspector, I'm going to click Y to install the dependencies. Let me just fast forward this a bit 'cause it can take a couple of minutes. Alright, so we have on port 3000, the MCP Inspector. So now we want to connect to our SSE server, which is running on port 8082, which is currently running. So let's go and click connect and let's check out the tools that this MCP server exposes. Going to click here on list tools and we have here two tools, list doc sources, which is going to show us what is the URL to the llm.txt file. So we can then make HTTP request and fetch the information and scrape that page. So you can see this is the URL over here and the other one is fetch docs. Let me try this with the llm.txt, which is going to receive a URL and it's going to retrieve all the content from that URL by scraping it. And what we can see here is the content of the LangGraph llm.txt file that we scraped with the fetch doc store. And we can see here all the URLs from the llm.txt and the agent is going to then extract those URLs and it's going to run the fetch docs for those URLs to get the relevant documentation dynamically. Cool. So let's go now and integrate everything with Claude desktop. So I'm gonna go to settings and I'm going to show you that I don't have any MCPs configured right now. So you can see my MCP servers is empty and I want to show you what is going to be without this MCP. So I'm going to ask a question, what is LangGraph memory? So now Claude Desktop is going to answer from its trained data and what it has on memory on LangGraph. So notice it may seem like a correct response, but eventually this response is not grounded in real time data and in the latest because it is dependent on the data that the model was trained on. Alright, let's continue the generation and let's check out the response. And by the way, this response actually may seem okay, it will go stale very, very quickly, especially when the rapid pace of LangGraph and LangChain packages are being updated. Cool. So now we want to add to Claude Desktop, the MCP server we just ran. So let's go again to settings and to developer and open the MCP settings config file. And here we want to paste in the snippet from the repo telling the client how to run this MCP server. So let's scroll down a bit and we got this snippet over here and let's simply go and paste it. Here we are running our server with UVX and the arguments we're going to tell it to run it from the MCP doc directory. The URLs that are going to store the llm.txt are going to be for the LangGraph documentation over here. We can change it if we want. The transport layer is going to be via stdio and the port is going to be 8081. And notice that the transport layer when we ran the server earlier was SSE, but now we change it to stdio. So both will work. Now it's time to restart Claude Desktop. And let's see if our MCP server got loaded. And we can see we have some errors here and for some reason we can't run UVX. So we get an ENOENT error and we can open the error logs, have some indication of what went wrong and if we'll scroll down a bit, we can see that we have a problem running the UVX command. So to do that, I'm going to go back again to my repository. I'm going to activate my virtual environment and I'm going to run the command which UVX to give the full pass of the UVX executable. And I'm going to put that over here. And now let's go and let's restart Claude Desktop again. All right, so it seemed to load the MCP server and if we'll click on the icon right over here, we can see we have the llm.txt MCP server. And as a sanity check, let's go and check out the settings and the MCP server we have here to see that everything is okay and this looks fine. And let's try to run what is langgraph memory? Now we can see from the response that nothing really changed here. So I have an issue with my MCP server and clicking on the tool button here shows the tools that our server exposes. And this looks fine. And after debugging it a bit offline, I figured out that when running the UVX command, I needed to specify the absolute path to where the code is stored because I don't know from which directory it's being run. So I want to give the absolute path here. All right, so let's go and restart Claude and let's see if it fixes the issue. Let's now write what is LangGraph memory. And boom, we can see now we are trying to activate the tool here. Now notice the tool we're trying to run here, Claude Desktop, the agent is trying to run the list doc sources tool. And because we initialized this code with the LangGraph documentation, so that's the built-in URL when we initialize the server, we are not running it with any arguments and we can see now the results that we got from this tool. So here we got that the LangGraph URL, the URL to the llm.txt is this here URL. And now the next tool that should run if everything in the agent is working correctly is going to be to scrape that URL data. So let's go and see what happens. And cool, now we're trying to fetch the documentation with the URL we got from the previous tool. So you can see that the URL we're passing into this tool is going to be the LangGraph llm.txt So I'm going to allow it and we're going to scrape the information from there and we're going to see all the information from the llm.txt. So let's go and allow it. And here we can see the result, which is the content of the llm.txt, which we script. So the content here is simply the topics and the URLs for those topics. I'm not going to show everything here, but the next step that the agent is going to do, it's going to find the relevant URL that we need to scrape. So the URL that talks about memory and it's going to scrape that URL again. So let's see if it invokes now the fetch docs tool again, however, with a different URL. And you can see this is exactly what's happening here and we are going to scrape now the langgraph/concepts/memory. So let's go and allow it and get the content, real-time content, this time of memory. So now it's going to fetch the information and here we have now the summary of what's LangGraph memory, however, this time it's grounded in real time information from the official LangGraph documentation and we got it in real time with the MCP server. So this is very cool and it helps us ground our answers in the real, real time documentation. Very cool in my opinion.