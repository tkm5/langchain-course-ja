Hey there, Eden here. And in this short video, we're going to be implementing the generation node. And the generation node is going to be the last node that is going to be executed. We execute this node after we already retrieve the information, the relevant documents, after we filtered out the documents that were not relevant to our query, and even performed a search for the question that we want to answer. So after we have all the documents, we can augment the original query. And now it's time to generate. So now it's time to simply stop everything and to send it to the LM to answer it. So this is what we're going to be implementing in this video. We're going to leverage the react prompt from the link chain hub. We're going to define a generation chain and to create a node that will call this chain. And of course we're going to be writing tests. All right let's go to the code. And we want to create a new file under our chains module. And we'll call it generation. And here we'll start with the imports. We want to import the hub from the chain because we're going to download the prompt from it. And we want also to import the str output parser, which is simply going to take our message. And it's going to get the content from it and turn it into a string. And we also want to import chat OpenAI. And let's create an LM instance from it. And now we want to pull the prompt from the LinkedIn hub. And this is a very standard racket prompt. That lens, smarting from the LinkedIn team, wrote, giving the LM the role of an assistant for question answering. Plugging in the context, which is going to be all the documents retrieved or web search that we saw from the earlier stages, and of course, the original question. So a very standard react prompt. So we're going to be using that. Our chain is going to be standard as well. We're going to create a generation chain where we pipe the prompt into the LM, and then we pipe the results into the STR output. So once we invoke this chain with the documents and the question, we're supposed to get the answer that we want. Alrighty. Let's go now and write some tests. So first I want to import the pretty print. And I want also to import the generation chain. And now let's go to the bottom of the file and let's create a test called test generation chain. And to be honest this isn't going to be an actual test that we are certain we're simply going to run this chain on a topic and we're going to see what's printed. It's just to give us a sanity check to see everything is working as expected. So we'll be querying agent memory and we'll retrieve the relevant documents. And now we want to run the generation chain with the context to be the documents we retrieved. And the question? The original question. And let's run it. And we can see it passed. And it gave us some summary about agent memory. So this looks fine. And we can even go to locksmith and to check out what happened. We can see we have here the retrieval. And we can see the documents that we retrieved from this query. And we can see even the documents content. And we can see it's relevant to memory. And if we go to the runnable sequence we can see the um the question that we asked. We can see it right over here. You're an assistant. And then we plugged in the context and we gave it the question of agent memory. And we got a response back which we output parsed into a string. All right, let's now run all the tests just to see that we didn't break anything. And we can see all of the tests pass. So I love seeing those green fee marks. Let's go and add a new file under nodes. And we want to call it generate. And here is going to be generate node. And you're guessing it. This node is going to simply take the question and take the documents from our state and simply run the chain. So it's going to be very straightforward. So all of the things we're doing here we already saw, but the last thing we're doing is updating the generation key in our graph state to be the generation the answer that the LLM responded.