Hey there, Eden here, and in this video we are going to do some boilerplate setup and we're going to set up the environment for this project where we are going to be showing an entire RAG pipeline from ingestion to retrieval. So let's go and let's set up our environment. So here I'm in the LangChain repository and the branch we're going to be working on, on this section is called project/rag-gist. This is here the project and hopefully I'm going to be adding a much nicer readme file that is going to explain what's happening here. Anyways, here we can see this entire code, which we're going to be implementing. And what's interesting is to go here to this commit list. And here we're going to see every commit for every lesson that we're going to be having now. Now I want to go now to the first commit, which you can see right over here where I added a bunch of files and this is going to be our starting point. So we want to go and clone the repository and switch to this branch of RAG gist and specifically to go to this commit. So let's go and let me open up a terminal and let me go and cd into desktop. So now let's go to the repository and let's go and copy here the URL of the repository and let's go and write git clone and then paste here, this repository. So now we can see, we cloned the repository. Let's cd into langchain- course. This is the repo. And here now I want, let's clear it out. So now let's go and switch to the branch that we want in the exact commit of the initialization. So let me write git checkout -b. That branch is going to be project/rag-gist. Now here we want to paste in the hash of the commit we want to go to. So let's go now to the branch rag-gist to the commit list and let's go and we want this initial commit. So let me go and copy this hash and let me paste it and we switch to a new branch project/rag-gist. So now let me fire up cursor and we can see cursor now is with the default configuration. So let me open the sidebar and here we can see we have all the files here. And if I'm going to go and click here this icon and we now have all the GIT information. And we can see now we are in the commit of initial commit. So this is exactly where we want to be here. So let's examine now the files and let's see what's happening here. So we have the git ignore file for files we do not want to commit and to track in our GitHub repository. Let's make sure that we have here .env. This is good. And here we can see we have the Python version I'm using. Here we have a boilerplate for the ingestion.py file. And notice here we have here an error. And this is because we haven't yet configured the cursor IDE to the virtual environment that is going to have all the packages installed. We're going to do it very, very soon. Here we have a medium blog that I put here as a TXT file and I simply went to Google, simply search for what is a vector db medium and went to this Medium blog here. I'm going to link everything in the videos resource of course, and I simply took all of the text here and pasted it. So this is what I did here. So this is an article regarding vector databases and now this is its text format. So let's go now and check out the pyproject.toml file. And here we have a list of all the dependencies for this project. So let's review them for a second. We have black, we have isort for formatting and we're going to install langchain. And don't worry about these versions. We are going to be installing a much later version of langchain, which currently is version 1.2 at the point of filming the video. However, when you are going to be installing it, you're probably going to have a new version and I want to promise you we're going to be having up-to-date code and I'm going to refill the videos if needed. So I do this continuously. Actually this video is a refill. Anyways, we are going to need langchain community for community contributed code. And for here we're going to be using it for a certain document loader that we're going to be using. We want to use langchain-OpenAI. This is the vendor I'm using now for the embeddings model and for the LLM we're going to be writing. Of course you can choose any LLM you want. Here we are installing langchain-pinecone and this is the integration of the pinecone managed vector store, which we are going to be using in this section. And we have here langchain hub. Actually we don't really need it, so don't worry about it. And we have here python-dotenv. So this is going to help us load the environment variables. And here we have the uv.lock file with all of the exact langchain versions that I installed when recording it. Alright, so now let me go and actually delete this file because I want now to create it again to have the latest versions at least at the moment. And here it's December 15th, 2025. You're probably going to have new versions. So let me open the terminal and here let me write uv lock. And this is going to take the TOML file and it's going to create a new lock file with all the new packages and dependencies that we need. Let's go and look for langchain. And here we can see the langchain version is 1.2.0. And this is good and this is currently the latest versions. All right, so the uv.lock file has only the information about the package. We want to install them also in our virtual environment. So let me write uv sync. And this is going to be installing all the dependencies from the uv.lock file. And you can see right over here, it created a .venv directory, which is untracked by Git because of our gitignore file. And now we installed all the dependencies in a new virtual environment. So if I'm going to open a new instance of the terminal in cursor, it's going to automatically load this new virtual environment. So this is great. Let's go now back to the ingestion.py file. However, here we can see we still have an error and this is because we haven't yet configured the IDE. So let's go and configure the IDE to the virtual environment. So, inside the virtual environment I'm going to write which Python3. And this is going to output the path to the interpreter with all the dependencies installed. So I'm going to copy it. Let's me go command+shift+P, let me go write select interpreter and here I want to enter an interpreter path. And here I want to select the recommended interpreter and we can see it fixes the issue. If it cause you problem, you should go and write Command+Shift+P and in the select interpreter, you can enter the path to the interpreter we copied. Let's now run everything as a sanity check and we can see it's printing this ingestion. Let's go now and create a .env file. And here we're going to be storing all the environment variables and all of our API keys. So let me paste in my OpenAI API key like we did in previous sessions and do not worry about my API key. Of course, I'm going to revoke every API key you see here after I finish filming this video. And let me now paste all the LANGSMITH related environment variables. So we have the LANGSMITH_API_key for the tracing. We have LANGSMITH_PROJECT RAG GIST so we can see it in the LANGSMITH UI. And we have LANGSMITH_TRACING=true. So now we want to go and configure our vector store. So we are going to be using pinecone. And Pine Cone is a managed vector store, which is cloud-based. And in the future videos we're also going to see open source alternatives and pinecone have a free tier, so it's more than enough for this course. So let me now open up my browser and let me go to pinecone.io. And here let me go and log in. Alright, so here we are in a place where we can see all of my indexes. Of course you should have an empty indexes directory. So I want to go now and create a new index. Let's call this index medium-bogs-embeddings-index. And you can call it in any name you want. We are going to configure it in the environment file. So let's go now and in the configuration we have custom settings and here we can enter manually the dimension of the vector store. So this is the lengths of the vectors that are going to be stored. And we can see we have a bunch of vector types, we have Dense, we have Sparse, we're going to go with Dense. And we have here the metrics. So those are the metrics to calculate the similarity between vectors. We have COSINE which is the default. We have Euclidean, we have dotproduct and we will be elaborating on that later in this course. And this is if you want to go with custom settings. What I like to do is I want to go and find the embeddings model that I'm using. And here we are going to be using at least me in the video text-embedding-3-small by OpenAI. And here we can see this is the default configuration and here we can see the dimension is 512 and I'm going to select it to be 1536 because I want it to hold more information. So the longer the vector, the more information, semantic information and semantic meaning it can hold. So this is the general rule of thumb. And here in the capacity mode I'm going to go with serverless. We have also an option to have dedicated read notes and maybe I'll dive into vector store configurations in later sections. Here we have the cloud provider. Now I do not care which cloud provider I am running for this tutorial. However, for enterprises who has compliance and privacy restrictions, they sometimes want to run on a certain cloud provider and pinecone supports the major three providers. We can also choose the region which our vector store is going to be deployed. I'm going to go with the default one. Here, we can see we have also in Ireland, in Oregon, and this is important when we deploy an application to production. We want our vector store to usually be in the same region as our RAG application because our RAG application is going to be making requests to the vector store. And if it's not in the same region then we're going to have egress cost. Anyways, I do not want to complicate things right now. We're going to be discussing those kinds of trade-offs in the production section of this course. And I'm going to go and create this index. So right now we can see that the index is created. So let me go and copy here this name and let me write here the environment variable of index name and let me paste it in. And I also want to go and create an API key and use it. So here I have my already created API keys. Let me go and create a new API key. Let me call it rag-gist-video and let me copy it and let me go and paste it under the name of PINECONE_API_KEY. Now the name PINECONE_API_KEY here is important because this is what langchain is going to be looking for when it's going to be using the langchain pinecone integration. So it's going to be looking for the environment viable with this exact name. Cool, so let's go now to the ingestion file. And now let me go and write print and let's see that we managed to successfully load all the environment variables. So for example, let me go and access OS ENVIRON 'PINECONE_API_KEY' and let me run it and we can see we get here an error. This is because I did not save the .env file. Let me save it and now let me run it again. Boom. We can see it's now printing it. Cool. So we finished initializing our environment. This was the boring part of the course and in the next video we are going to ingest our blog here. So we are going to run the ingestion pipeline, which is going to take our blog, it's going to chunk it up into smaller pieces of text, it's then going to embed every piece of text and turn it into a vector. And then we are going to take those vectors and we are going to store all of them in the PINECONE Vector store.