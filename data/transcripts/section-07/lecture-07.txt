All right. So let's go and implement our retrieval pipeline. So I'm going to start by creating a main.py file. And here is going to be the retrieval part of our project here. So let's go now and start with the imports and the initialization of the object that we'll be using. And let's start. So first I want to import the OS module because we're going to be accessing environment variables. And now let's go and import load env because we're going to be loading all the environment variables from the env file over here. Let's go now and import from the chat prompt template. Because this is what's going to form the react prompt we're going to be using. We want to import human message. This is what is going to be invoking the pipeline. And let's go and import from OpenAI the chat OpenAI for our ML model and OpenAI embeddings for our embeddings modules, which we covered in the previous section. And also let's go and import like previously explained, the pine cone vector store. And let's start with the initialization of all of the objects. So first we want to load all the environment variables. And let's now go and print initializing components. So now we're going to be initializing every one of those objects here. So let me go now and run everything as a sanity check. And we can see we are printing this indeed. Cool. So let's start with initializing OpenAI embeddings. And let's also initialize the LLM. And here we're using the link chain OpenAI defaults. And we'll see which LLM is going to be using. Let's go now and initialize our vector store object. So in order to initialize it we need to give it the index name which we created earlier in the videos. And we need to give it the embeddings module, very similar to what we did in the previous video. And now we want to use this vector store and we want to use its searching capabilities. So we want to be able to make similarity searches to find these relevant contexts in direct pipeline. So if earlier let's go to the ingestion.py file we use the from document method. Let's go back now to main.py. Now we want to go and we want to take this vector store. And it has a method which is called as retriever. And this function is going to return us. Let's go and open the docs here. It's going to return us an object of vector store retriever. So this vector store retriever from link chain I'm not going to go too deep into this in this section right now, but it's going to give us a vector store with searching capabilities that the vendors of the vector store implemented. And eventually these vector store retriever class is going to have this searching capability. It's going to have a search function, which we're going to be using now. So when we initialize it we're going to give it search queries. K equals three. So this means every time we want to search in the vector store and get those relevant chunks those relevant contexts. I want to be limiting it to only three top documents that are going to be used. So if there are maybe ten relevant documents, it's going to sort them by order of importance of what's the most relevant and what's the least relevant. And I want to take the top three. And let me now go and initialize the prompt template here. So I'm going to initialize it with a very simple prompt. But it's a very powerful prompt. The prompt goes as follows. Answer the question based only on the following context. Here we have the placeholder for the context. Here we have the original question of the user and provide a detailed answer. So this context here. This is going to be the augmentation part of our prompt. So the question here is going to be our original prompt. And this is going to be the augmentation. So from the retrieval augmentation generation of Rag. And now let's go and implement a very simple auxiliary function. So we want to implement format docs which is going to receive docs which are going to be linked in documents. And this function is going to take the documents that were retrieved from the vector store, and it's simply going to format them nicely into a nice string. So you can see all that it's doing. It's going to iterate through all the documents. This is the for loop over here. Every document we saw before has a page content. So we're going to take this page content which is a string. And we are simply going to append newlines before it. So if we have a bunch of documents we're simply going to format them as one string. And this one string is eventually what's going to be sent here to this context, right? So those are all the initialization of the objects, and those are all the building blocks that we're going to be using now. Cool. So now let's go try to run everything. Let's see that everything is going to be initialized. Cool. We didn't get any error. This was a sanity check. Cool. So now let's go and use our main runner here. Now I wrote if name equals main, we're going to be print retrieving. And the query is going to be what is pinecone in machine learning. So first let me go and show you what we're going to get if we're not going to be using rag. So here I pasted a raw invocation which is simply going to use the LM. It's going to send it in the human message, the content of the query, which is what is pinecone in machine learning. And then it's going to print it. So let's go and let me show you what we're going to be getting. So this is a raw invocation. We are not using rag. So here we can see the answer. A pinecone algorithm is a method used in machine learning to search for the best configuration hyperparameters. This is not the answer that we want, because we want to talk about pinecone as the vector store. All right. And let me even go to links. Let's go to our links project here that I initialized. Here we have the chain. What is pinecone in machine learning. And here we can see the answer. By the way the model that we're using is GPT 3.5 turbo. So just for the sake of it let's go and change here the model. Let's try to use model GPT 5.2. Now we can see the answer. Python is a managed vector store database using machine learning to store index. Okay. So this is a nice answer. So this is like the answer we wanted. And by the way the difference here and this is by the way one of the motivations of using Rag because GPT 5.2 was trained in 2025 and pinecone was much established by then. So we had lots of documents and lots of training data on pinecone as the vector store. And before that it was not so GPT 3.5. As we saw before, it hallucinated the answer. This is also something interesting to see. Cool. So now let's go and use a chat. OpenAI with GPT 3.5. And we saw what was the answer without using Rag. And now let's go and implement the Rag pipeline with all the building blocks. All right. So now let's go and implement the retrieval part of our pipeline. So first let me define a function which is called retrieval chain without link chain expression language. And this is a function which is going to receive a string, and it's going to return us a response of the LM. The way we're going to be implementing now is without using linked chain expression language, and this is simply to show you the flow and to show you what's happening under the hood. And this is to show you how we can do it without link chain and how it's going to be looking like. So let's go and start and implement this. And this is the most naive implementation. After we're going to be doing that, we are going to implement it with linked chain expression language, which is going to yield us with a much more elegant solution and much more capabilities. So here in the description this is going to be a simple retrieval chain. It's not going to be really a chain. It's simply going to be a bunch of function invocations. And here we're going to manually retrieve the documents. We're going to manually format them. And we're going to generate the answer. So in this implementation we have a bunch of limitations which are going to be solved. In this video we are going to invoke everything manually. We don't have any streaming support. We don't have any async support. It's hard to take these and compose it in other chains. And this is going to be error prone and it's going to be harder to maintain. And we're going to see how we're going to be solving every one of those limitations in the length chain expression language implementation, which is going to be in this video. So the first thing we want to do is we want to take the user query the question, what is pinecone in machine learning. And we want to get the most relevant documents. We want to perform similarity search in the pinecone vector store. So when we're going to be using our retriever dot invoke. So retriever right over here is actually a link chain runnable. So if I'm going to actually go over here and this is going to return us a vector store retriever object. This object is inheriting from base retriever and base retriever. Here is a runnable. So this is going to have the invoke method here. Now when we're going to be using a vector to retrieve our in length chain because it has an invoke method, its invoke method, it's going to run something. It's going to run a bunch of things, but eventually it's going to run this function, get relevant documents, which is going to take our query, and it's going to return us a list of documents which are going to be the most relevant documents. So these get relevant documents. This is eventually going to be implemented by the vendor. Pinecone have their own way of doing things. It's going to use the pinecone SDK. If we're going to be using chroma it's going to be using another implementation. And this is eventually what's going to be running. And we have also an async version of getting the relevant documents. But eventually when we are going to be running this invoke method, it's going to be going and doing this. And we're going to see this in the length trace by the way. And let's now assume. So this is going to magically give us the most relevant documents from our original query here. And because we initialized it with k equals three. So this means that this is going to be a list of three link chain documents. So we have now the relevant documents. Next we want to go and let's format them into a string. So we're going to be using the format docs. And it's going to give us here the context which is going to be a string. Cool. So right now we have the question we have the context for this question. So now we want to go and we want to take this prompt. And we simply want to plug the value of the context and the value of the questions here. And we want to go and send this to the LLM. So now we are taking the prompt template. We are going to use the format messages. And here we're going to provide it with the context equals context. And the questions equals query. So question here is matching. Now this placeholder here in the query is the value that we're going to be giving it. So now this is going to give us a list of messages holding one message, which is going to be everything we want to send to the LM. And now let's go and invoke our LM with the list of messages, which is going to be one message. And we simply want to return the content of that response here. So this is the entire implementation the manual implementation of the Reg pipeline over here. All right. And now let me paste in the snippet which is simply going to print that we want to invoke the implementation without an expression language. We're going to be taking this function. And we're going to be running it with the user's query. And we're going to be printing. Now the result nothing special over here. All right. Let me now. So let's go now and run everything. So this is the raw invocation here we see we get the wrong answer. And let's see the question answered with rag. And here we can see pinecone is a fully managed cloud based vector database specifically designed for businesses and organizations looking to build and deploy large scale machine learning applications. So this is the answer that we wanted working with Rag here. So now I want to show you in debug. And let's see step by step what's happening here. So let me go to the function here. And let me now put a breakpoint. Let's go run it in debug. All right. So here we can see we have now the query what is pinecone machine learning. Let me now go and execute this. So here we can see. Now we have documents variable. and it's going to contain a list of three link chain documents. And every document here is going to have its page content attribute here, which is going to be the data of this document here. So these are the most relevant documents that pinecone found for us. Now we want to take those documents and now we want to format them as strings. So let's go and run this format docs function. And now we have the context variable. And now we can see this is a string okay. So now let me copy here this expression. Let me open it here and let me paste it. And we can see it's a very big string of all the documents combined. So now we want to go and we want to initialize the prompt template with the context of the documents. Here we can see all the documents. And we want now to give it the query. And we want to initialize it with the query. What is pinecone in machine learning. Let's go and do that. Let's go and look for messages. So it gave us a messages list with one message. And each content is the prompt we had before. Answer the questions based on the following context. And then it pasted the context and gave the question here. Cool. And let's go now and invoke now the LM. And this is a regular LM invocation right now. And we get here a response. Let's go and check out this response. The response is an AI message and its content is the answer. So now I want to show you the Lang Smith trace. And because here in these functions we did not really use a Lang chain chain. And we did not use the chain expression language, then we simply used all linked chains components separately. So all the traces are not going to be organized under one component of a chain, and this is not convenient at all. Okay, so now let's go and review the traces here. So let's take the first step of retrieving the relevant documents. So here we can see we have a vector store retrieval right over here. And this is the run of this line over here. So here we can see the input is what is pinecone in machine learning. When we ran it in evoke, the output we get was those three documents of the data that are the most relevant documents. So this was the similarity search that pinecone did for us under the hood here and here we can see we have the text. So this is the text of the document. And here we even have the source which I did not show you in the debug console. So now after we did that we went we formatted nicely. This is nothing to do with Lang Smith. And here we simply populated the prompt template. And this is not going to show in Lang Smith as well because it's not under the chain. And finally, we invoke the LM with all the relevant data in the augmented prompt. So we can see the LM invocation right over here. So the final call to LM is the original prompt. Here we have the context. And here we have the question that we plugged in. And this is the answer that the LM returned us. So we can see tracing this and debugging this is going to be a bit harder. All right. So what we did now in the implementation is implement everything very very naively. And it has a bunch of disadvantages. But the main disadvantage is that it's going to be very hard to trace it. So in the next video we're going to implementing the same functionality but binding everything under a link chain chain with the link chain expression language. And this is going to be a much better implementation. And we're going to see all the reasons why it's better.