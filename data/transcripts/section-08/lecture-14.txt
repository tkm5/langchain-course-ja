-: Hey there, Eden here, and in this video I want to talk about RAG applications in production. And specifically, I want to take the example I show in the course of the Documentation Helper, which takes the LangChain docs ingests them and create a RAG system on top of them. And I want to show you the project of Chat LangChain, which take this prototype that we build in this course and takes it to the next level because here it's implementing the paradigm, which is called Agentic RAG, which is implemented with LangChain, LangGraph, and it has a very elaborate system of curating our query and generating an optimized output that is very usable. I also like, by the way, the UI and the user experience of this application and I think it's a very good example. Now what's cool about this application is that it's open source and LangChain actually show you the code and you can deploy it in your use case as well. All right, so let's talk about chat.langchain.com. Now this is an application that LangChain has built, which is very similar to our documentation helper that we build in this course. And both applications provide the user the ability to chat with the LangChain documentation. However, here, LangChain took it to the next level and they have a very advanced architecture of a RAG system over here. Now they're implementing something which is called an Agentic RAG, which I talk about in my language course. Now before we dive in into this implementation over here, let me just show you how does it work. So we can ask questions about the LangChain documentation. So let's go for example and ask what is LangChain? And right now the system is going to generate a bunch of questions which are related to our original question. So the first question is to review the documentation and gather comprehensive definition of LangChain and then it's going to receive retrieve docs about that query. And it's going to do the same for two more queries. and the goal here is a heuristic that is going to retrieve better documents which are more relevant for us. So this is a very cool heuristic. So for each one of those subqueries, which are derived from our original query, we're going to perform some semantic search and we're going to retrieve documents that are going to help to answer this question and that's going to be the selected context. So it's going to be all the documents retrieved and going to be probably filtered out or re-rank by relevance. So once we have the documents that are going to help the question, we are going to augment the query and then the system is going to produce the answer. Of course, once we produce the answer, we are going to also output what are the sources. Now at any point of our time of the run, we can check out the relevant context over here, for example. So those are the retrieved documents and you can see here that the UI here is very, very natural and we can really understand what the system is doing and how is it built and that there is no magic over here. So this really creates trust between the user and the system, which is very important for Generative AI application. And this field is actually called Generative UI and it is the art of creating very smooth and nice user interfaces and user experiences for generative applications. And I do talk about it on the production section of this course. All right, let me test the core reference resolution here. So I'm going to ask who created it. Now we can see we got relevant subqueries over here, which is to understand that I am referring to LangChain And by the way, this application also have the ability to search online, but we're not searching online for this kind of query. All right, so now we get the answer that LangChain was created by Harrison Chase. So we can see here the documentation link and yeah, we got here the answer and we can see that the core reference resolution is working. All right, so this application is actually open source. So let me just go and search it up on Google. I'm going to write chat LangChain GitHub and this is this repository right over here. So basically it's built with the stack of LangChain, LangGraph and NEXT.js, this is going to be the front end and you can check out the code. And if I'll go here to the back end and under the retrieval graph, we can check out the prompts that are comprising the application. And this is going to be a multi-agent system. So let's go to Prompt.py over here. And here we can see we're downloading a bunch of prompts which are going to be used in the application, and we download them from the link and have like we did before. We can see we have here the router prompt and I explained the router concept in this course. We can have here the generate queries prompt, which is going to generate from the original query of the user sub queries to search for. We have a bunch of more prompts and I highly recommend you checking them out because they're very good examples of proper prompt engineering. All right, so those are the prompts which are going to be used in the application. So let's check out now the rest of the logic here. And again, we're not going too much in depth here. I'm just trying to show you the idea. Let's go now to the retrieval graph and let's go to here to graph.py. And here we have the actual usage of those prompts. And here we have an example of a multi-agent system, which is implemented with LangGraph. So right over here you see a LangGraph implementation. Now don't worry that you don't know what's LangGraph or what are multi-agent systems because I do cover the introduction for those topics in this course. But I go over the essentials and in-depth in those topics in my LangGraph course, which builds right on top of this course. The only thing I wanted to show you in this GitHub repository and this example, is that we have some advanced logic here, which is going to optimize our results. And this is how the LangChain team took the idea of the implementation helper and took it to the next level and implemented something which is production-ready, which produces high quality results.