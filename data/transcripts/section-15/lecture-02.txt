-: So in order to understand MCP in depth, we really need to go back a bit and to talk about the history of LLMs and AI applications in general. So let me remind you what are LLMs. LLMs are simple token generators. They're guessing one token after the other and they're simply text generators. And this is something which is not that obvious because nowadays, with all the agentic behavior, people think that LLMs have superpowers and they can do tons of stuff, and this is not the case. LLMs can only output text. Or maybe if it's multimodal LLMs, they can also output pictures and other formats, but they certainly cannot go and perform actions. So those extra capabilities like searching the web, performing deep research or invoking a Python function, those are external tools that are integrated into the application which is running the LLM. So for example, in ChatGPT, you use maybe the ChatGPT application in your desktop, or you use the ChatGPT web application, so it has the LLM wrapped inside an application. Software engineers wrote those applications. Let me remind you how tool usage works. So tools like web searching is external code that are not part of the LLM, that software engineers wrote. So if you use ChatGPT and you toggle the web search option, so the software engineers that work at OpenAI, they wrote this functionality. And how do we get this ad hoc behavior of tool usage with LLMs? So what's happening underneath the hood, and this is probably what's happening with OpenAI and all the other chat applications that are leveraging LLMs, is that we leverage tool calling. So remember the LLMs are text and token generator. So what happens here is that we have a very fancy system prompt, yes, this all boils down into a system prompt, which instead, let's say in a question like, "What is the weather right now?" Instead of generating this answer for those kinds of use cases, it won't say that the weather right now is 25 degrees Celsius, but what it would do, it would generate the text, get weather, and open parenthesis with the arguments of the city that we want the weather to, okay? So instead of generating the answer for this and hallucinating it, because it doesn't have access to real world information, then it simply generates the tool call, the tool invocation. And this tool call is going to be in a very specific format that the vendor designs, which is going to be very easy to parse. It's going to be easy to parse the functions that need to be called. It's going to be easy to parse the arguments which we need to call the function with. And there are many variations of tool calling. Each vendor implements it differently. But it all boils down into a very special system prompt. And we reviewed in my launching course the react prompt, which is an example of one. And this is basically what's happening underneath the hood. The LLM application, for example, ChatGPT, then takes this output, it parses it, and if there is a tool call, it simply goes and invoke the functionality that the engineers wrote. So it can have, for example, a web search tool, and the ChatGPT application is going to be wrapped in a very special prompt that is going to output, when necessary, the invocation of a web search with the user's query. And for example, if I would go and ask, what is the stock price of NVIDIA, it would go and generate the tokens of search on the web, of web search and query to be NVIDIA's stock price. So after the ChatGPT application perform the tool call, then it will generate another LLM call with the result of that tool call and the original user's query. So this is the basic functionality of almost every given agent. And also, it's important to note that LLMs are statistical creatures, so they are guessing the token one after another. So this tool calling, this tool calling mechanism, which the LLM is going to output the correct tool that we need to invoke with the correct arguments, it does not work 100%, but it works a lot of the time and most of the time, is actually pretty good for agentic applications. So to summarize, LLMs are simple token generators and token predictors. And tool calling capability is actually ad hoc behavior that we add in the application layer. So now, let's go back to MCP, and MCP lets us focus on writing those tools and exposing them in MCP servers. So those tools that we write, they can be used on all other applications that supports function calling. So they can be supported by ChatGPT at the end. So they announced that they are going to support MCP. They can be supported by the cloud desktop or Cursor.