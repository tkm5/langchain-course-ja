Messages are the fundamental units of communicating with llms. When working with chat models in Linkchain, they carry the input we want to send to the LM and the output we receive back. Every message has two essential parts. It has a role and it has content. The role tells the model who sent the message. The content holds the actual information, which is typically text, but can also include things like images, videos, or other data for multimodal models. Linkchain standardizes these messages, allowing us to work with different chat model provider using a consistent format. So this means it doesn't matter which LM we're using. We still use the same interface, which is very, very convenient. So let's take a look on the different roles messages can have. We have the system role which is used to set up the AI's behavior or to provide initial context. For instance, you might use a system message to tell the model to act as a helpful assistant or to summarize complex documents concisely. Not all models handle system message, by the way. Some use a dedicated parameter. Other integrated into the message history. And few don't support it at all. However, Linkchain tries to handle these variations for us and it's going to create one single interface. The user role represents input coming from a person or system interacting with the model in Linkchain. We'll use the human message class for this. When we send text to a chat model, like asking a question or giving an instruction, we wrap that content in a human message class. Linkchain even offers us a shortcut. If we want to invoke a model with just a string, it's automatically being treated as a human message. The assistant role is the model's response. This output is representative by the AI message class in LinkedIn. When the model answers our question or completes a task, its response comes back as an AI message object. These messages contain the main content generated by the large language model, along with other information like metadata or tool calls, token usage information, and unique identifiers to help us in debugging. Tool role will elaborate this when we talk about agents and function calling. But overall, it's a way for us to tell the AI what was the result of the tool execution. For example, if we invoked a tool that gets us the weather in somewhere, then we're going to wrap that answer in a tool message object. And that's how we're going to tell the model that that's the result of the tool call. So it can give us the final answer. But don't worry if you don't fully understand this part. We're going to elaborate a lot on this topic in the course. The order of these messages is important for a coherent conversation, and a typical flow involves alternating between user input and assistant responses. So it's going to be human message, then AI message, then human message, then AI message, and so on. If tools are involved, we probably see them in like and if tools are involved, we'll see this in human message. AI message with a tool call, then a tool message with the tool result, then an AI message using the tool result, and so on and so on. And just to conclude, link Messages object provide us with a consistent way to build these conversations, abstracting away the specific message formats required by different providers.