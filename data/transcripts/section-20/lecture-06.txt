So this video is going to be all about memory and specifically all about memory in lecture one. As far as user interactions when it comes to LMS, LMS are stateless. So this means that the LMS don't save any information in the conversation that occurred in the conversation earlier. So it's stateless. Let's take a look at an example. Now if I'm going to ask my large language model that is based on the long chain documentation, who created long chain, then I would get a correct response that it was created by Harrison Chase. But if I'm going to follow up and ask, do you know any YouTube videos related to him, then the answer that I will get is, I'm sorry, I don't know who you're referring to. Could you please provide me with more context or clarify who is him? So the concept that we're discussing right now in the formal terminology is called a coreference resolution. Basically, it's the task of identifying all expressions, words or phrases in a text that refer to the same entity or concept. So in other words, it's the process of identifying all the instances where different words or phrases in a text refer to the same thing. And in our example, him referred to Harris and Chase. Now the LM was not able to do it because it didn't have a state, so it wasn't able to make a co-reference resolution. However, if the LM in the prompt will get the state and the chat history, then it will be easily able to make the co-reference resolution. And this is the underlying base to all the solutions that blockchain is currently supporting. For memory, we simply find sophisticated ways to pass into the prompt some data, some information that will help the LM to make some co-reference resolution, like in the example we see right now, the prompt that we're giving it is given the past conversation. Answer my question now. In the past conversation, we're simply going to answer an interaction between us and the bot. Something about I like drinking cold brew coffee. Where can I get it? And the last question is where else can I find that? And by that I am referencing the cold brew coffee. And also the LM will know that I don't want to drink at Starbucks, nor at Coffee Bean, and the LM can easily process that and can make some co-reference resolution. Now, you may notice that if we have a one hour conversation, then we are in a problem because we have too much data to put into the prompt. We know that there is a token limit, and if we have a very, very long conversation, then it will definitely exceed this limit. Okay. So now let's see how long chain resolves it. And let's take a look at all the long chain offerings regarding memory. Now this video is a theoretical video. So I'm not planning to show you any live examples of how to use them. They're pretty straightforward and we're going to see examples of them in the rest of this course. So don't worry about seeing implementations of those classes right now. What I want to demonstrate to you is the strategies that are used in order to resolve this token issue when it comes to memory.