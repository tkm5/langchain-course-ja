-: So I remind you this is the implementation of our SSE server and in order to run it all we need to do is run the command uv run then servers and the weather_server. So we can see now it's running on port 8000. And now I want to create a new MCP client that is going to use this SSE server and it's going to use the STDIO server we had from before with the math operations. So let me open another terminal and let me run the math server. So uv run servers/math py and it's running around. So let me go now and create a new file and let me call this file langchain_client.py. And the reason why I'm calling it that is because we're going now to implement a langchain multi-server client. So the langchain multi MCP servers client is a new client that langchain wrote for us that is able to connect to multiple MCP servers. So remember I told you there is a one-to-one connection between a client and an MCP server. So this is still the case. Langchain abstracted for us and inside it we're going to have multiple MCP clients but we can use that to easily connect to multiple MCP servers without explicitly writing the clients for each one of those servers. So to do that we'll start by importing from langchain_mcp_adapters.client. We want to import the MultiServerMCPClient. Let's go and import from langgraph.prebuilt, the create_react_agent like we did before. And let's import OpenAI, ChatOpenAI exactly like we did before. And of course we want to implement from dotenv load_dotenv. And we also initialized the llm. And let me open a new terminal and let's run this file for a sanity check. uv run langchain_client.py and we can see it ran successfully. Let's also define a async function, main, and let's write here print("hello langchain mcp") and we're also running it with the asyncio. And of course I need to import asyncio. And yeah. So this is the boiler plate code. Let's run it as a sanity check. uv run langchain_client.py. So this is working as expected.